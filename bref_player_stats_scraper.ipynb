{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web page scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create url templates for each kind of stats\n",
    "per_g_url_template = \"https://www.basketball-reference.com/leagues/NBA_{year}\\\n",
    "_per_game.html\"\n",
    "adv_url_template = \"https://www.basketball-reference.com/leagues/NBA_{year}\\\n",
    "_advanced.html\"\n",
    "tot_url_template = \"https://www.basketball-reference.com/leagues/NBA_{year}\\\n",
    "_totals.html\"\n",
    "per_36m_url_template = \"https://www.basketball-reference.com/leagues/NBA_{year}_\\\n",
    "per_minute.html\"\n",
    "per_100p_url_template = \"https://www.basketball-reference.com/leagues/NBA_{year}_\\\n",
    "per_poss.html\"\n",
    "\n",
    "# Put all the URL templates into a list\n",
    "url_template_list = [per_g_url_template, adv_url_template, tot_url_template, \n",
    "                     per_36m_url_template, per_100p_url_template]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask user to input start and end years\n",
    "# Also checks to see if entry is a number\n",
    "#try:\n",
    "    #user_start_year = int(input(\"Enter start year in YYYY format: \"))\n",
    "user_start_year = 2009\n",
    "#except:\n",
    "    #print('Enter a valid 4 digit year.')\n",
    "    \n",
    "#try:\n",
    "    #user_end_year = int(input(\"Enter end year in YYYY format: \"))\n",
    "user_end_year = 2023\n",
    "#except:\n",
    "    #print('Enter a valid 4 digit year.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year range accepted.\n",
      "Year format accepted.\n",
      "Year format accepted.\n"
     ]
    }
   ],
   "source": [
    "# Check if end year is after start year\n",
    "if user_end_year >= user_start_year:\n",
    "    print('Year range accepted.')\n",
    "else:\n",
    "    print('Year range is unacceptable.')\n",
    "\n",
    "# Check if formats are in proper YYYY format\n",
    "def check_year(user_input_year):\n",
    "    if user_input_year > 999 and user_input_year < 10000: # Then check if it's 4 digits\n",
    "        print('Year format accepted.')\n",
    "    else:\n",
    "        print('Enter a valid 4 digit year.')\n",
    "        sys.exit()\n",
    "\n",
    "# Check both entered years for formatting        \n",
    "check_year(user_start_year)\n",
    "check_year(user_end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store data before appending to Dataframe\n",
    "column_headers = []\n",
    "player_data = []\n",
    "# Create empty DataFrame for following functions to fill\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty DataFrames for each set of pages\n",
    "df_adv = pd.DataFrame()\n",
    "df_per_g = pd.DataFrame()\n",
    "df_tot = pd.DataFrame()\n",
    "df_per_36m = pd.DataFrame()\n",
    "df_per_100p = pd.DataFrame()\n",
    "\n",
    "# Create df_list of DataFrames for looping\n",
    "df_list = [df_per_g, df_adv, df_tot, df_per_36m, df_per_100p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column headers from each page\n",
    "# Assigns a new list of column headers each time this is called\n",
    "def get_column_headers(soup):\n",
    "    headers = []\n",
    "    for th in soup.find('tr').findAll('th'):\n",
    "        #print th.getText()\n",
    "        headers.append(th.getText())\n",
    "    #print headers # this line was for a bug check\n",
    "    # Assign global variable to headers gathered by function\n",
    "    return headers    \n",
    "    #column_headers = [th.getText() for th in soup.find('tr').findAll('th')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get player data from each page\n",
    "def get_player_data(soup):\n",
    "    # Temporary list within function to store data\n",
    "    temp_player_data = []\n",
    "    \n",
    "    data_rows = soup.findAll('tr')[1:] # skip first row\n",
    "    for i in range(len(data_rows)): # loop through each table row\n",
    "        player_row = [] # empty list for each player row\n",
    "        for td in data_rows[i].findAll('td'):\n",
    "            player_row.append(td.getText()) # append separate data points\n",
    "        temp_player_data.append(player_row) # append player row data\n",
    "    return temp_player_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function scrapes the stats data of one page and returns it in a DataFrame\n",
    "def scrape_page(url):\n",
    "    r = requests.get(url) # get the url\n",
    "    soup = BeautifulSoup(r.text, 'html.parser') # Create BS object\n",
    "    \n",
    "    # call function to get column headers\n",
    "    column_headers = get_column_headers(soup)\n",
    "    \n",
    "    # call function to get player data\n",
    "    player_data = get_player_data(soup)\n",
    "    \n",
    "    # input data to DataFrame\n",
    "    # Skip first value of column headers, 'Rk'\n",
    "    df = pd.DataFrame(player_data, columns = column_headers[1:])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(input_year):\n",
    "    first_yr = input_year - 1\n",
    "    season = str(first_yr) + \"-\" + str(input_year)[2:]\n",
    "    return season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function drops empty rows an columns, drops duplicates, and changes\n",
    "# % character in columns\n",
    "def gen_cleaning(df):\n",
    "    # Convert values to numeric values first\n",
    "    df = df.apply(pd.to_numeric, errors = 'ignore')\n",
    "    \n",
    "    # Drop columns with no data\n",
    "    df.dropna(axis = 1, how = \"all\", inplace = True)\n",
    "    \n",
    "    # Drop rows with no data\n",
    "    df.dropna(axis = 0, how = \"all\", inplace = True)\n",
    "    \n",
    "    # Remove duplicates player inputs; ie. players who were traded\n",
    "    # I only kept the TOT per game season values\n",
    "    #df.drop_duplicates([\"Player\"], keep = \"first\", inplace = True)\n",
    "    \n",
    "    # Change % symbol to _perc\n",
    "    df.columns = df.columns.str.replace('%', '_perc')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function scrapes player data from multiple pages by start and end years\n",
    "def scrape_pages(url_template, start_year, end_year, output_df):\n",
    "    count = 0 \n",
    "    for year in range(start_year, (end_year+1)):\n",
    "        url = url_template.format(year = year) # grab URL per year\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html5lib') # Create soup item\n",
    "        \n",
    "        # Check to grab column headers\n",
    "        if count == 0: # only append column headers once\n",
    "            columns = get_column_headers(soup)\n",
    "            count += 1\n",
    "            \n",
    "        # grab player data for each year\n",
    "        player_data = get_player_data(soup)\n",
    "        \n",
    "        # Create temporary DataFrame first for each year\n",
    "        # Duplicates are removed before putting into bigger DataFrame\n",
    "        # These duplicates come from players playing on multiple teams in one season\n",
    "        # This script only keeps the TOT output as Tm\n",
    "        year_df = pd.DataFrame(player_data, columns = columns[1:])\n",
    "        year_df.drop_duplicates(['Player'], keep = 'first', inplace = True)\n",
    "        year_df.insert(0, 'Season', get_season(year)) # insert season year column\n",
    "        \n",
    "        # Append to big DataFrame for detailed cleaning\n",
    "        output_df = pd.concat([output_df, year_df], ignore_index = True)\n",
    "        \n",
    "    # Do common, general cleaning practices\n",
    "    output_df = gen_cleaning(output_df)\n",
    "        \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herma\\AppData\\Local\\Temp\\ipykernel_12392\\2948804278.py:5: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df = df.apply(pd.to_numeric, errors = 'ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished per g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herma\\AppData\\Local\\Temp\\ipykernel_12392\\2948804278.py:5: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df = df.apply(pd.to_numeric, errors = 'ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished adv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\herma\\AppData\\Local\\Temp\\ipykernel_12392\\2948804278.py:5: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df = df.apply(pd.to_numeric, errors = 'ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tots\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'findAll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m df_tot \u001b[38;5;241m=\u001b[39m scrape_pages(tot_url_template, user_start_year, user_end_year, df_tot)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished tots\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m df_per_36m \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mper_36m_url_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_start_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_end_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_per_36m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished per 36m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m df_per_100p \u001b[38;5;241m=\u001b[39m scrape_pages(per_100p_url_template, user_start_year, user_end_year, df_per_100p)\n",
      "Cell \u001b[1;32mIn[21], line 11\u001b[0m, in \u001b[0;36mscrape_pages\u001b[1;34m(url_template, start_year, end_year, output_df)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Check to grab column headers\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# only append column headers once\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43mget_column_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# grab player data for each year\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m, in \u001b[0;36mget_column_headers\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_column_headers\u001b[39m(soup):\n\u001b[0;32m      4\u001b[0m     headers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m th \u001b[38;5;129;01min\u001b[39;00m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindAll\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mth\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m#print th.getText()\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         headers\u001b[38;5;241m.\u001b[39mappend(th\u001b[38;5;241m.\u001b[39mgetText())\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#print headers # this line was for a bug check\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Assign global variable to headers gathered by function\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'findAll'"
     ]
    }
   ],
   "source": [
    "# Fill each DataFrame with data scraped from their respective pages\n",
    "# Each print statement is a check for if any pages or functions give issues\n",
    "# Added timer to check how long this was taking\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df_per_g = scrape_pages(per_g_url_template, user_start_year, user_end_year, df_per_g)\n",
    "print(\"Finished per g\")\n",
    "df_adv = scrape_pages(adv_url_template, user_start_year, user_end_year, df_adv)\n",
    "print(\"Finished adv\")\n",
    "df_tot = scrape_pages(tot_url_template, user_start_year, user_end_year, df_tot)\n",
    "print(\"Finished tots\")\n",
    "df_per_36m = scrape_pages(per_36m_url_template, user_start_year, user_end_year, df_per_36m)\n",
    "print(\"Finished per 36m\")\n",
    "df_per_100p = scrape_pages(per_100p_url_template, user_start_year, user_end_year, df_per_100p)\n",
    "print(\"Finished per 100 possessions\")\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time elapsed :\" +str((end - start) / 60) + \" minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Auditing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all column names to see what needs to be cleaned\n",
    "\n",
    "print(\"totals\")\n",
    "print(list(df_tot))\n",
    "print(\"per game\")\n",
    "print(list(df_per_g))\n",
    "print(\"per 36 minutes\")\n",
    "print(list(df_per_36m))\n",
    "print(\"advanced\")\n",
    "print(list(df_adv))\n",
    "print(\"per 100p\")\n",
    "print(list(df_per_100p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Label columns properly by adding \"_tot\" to the end of some column values\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df_tot\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues[[\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m , \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m19\u001b[39m]] \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m----> 3\u001b[0m [\u001b[43mdf_tot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m19\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m[col] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tot\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m)]\n\u001b[0;32m      5\u001b[0m df_tot\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m21\u001b[39m:\u001b[38;5;241m30\u001b[39m] \u001b[38;5;241m=\u001b[39m [df_tot\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m21\u001b[39m:\u001b[38;5;241m30\u001b[39m][col] \u001b[38;5;241m+\u001b[39m \\\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_tot\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m)]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 7 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# Label columns properly by adding \"_tot\" to the end of some column values\n",
    "df_tot.columns.values[[7, 8 , 9, 11, 12, 14, 15, 18, 19]] = \\\n",
    "[df_tot.columns.values[[7, 8 , 9, 11, 12, 14, 15, 18, 19]][col] + \"_tot\" for col in range(9)]\n",
    "\n",
    "df_tot.columns.values[21:30] = [df_tot.columns.values[21:30][col] + \\\n",
    "\"_tot\" for col in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column titles again\n",
    "list(df_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop _perc columns from per_g and per_36m\n",
    "# Never mind, drop duplicates later on\n",
    "# Add _per_g and _per_36m to column values\n",
    "# Add _per_G to some values in df_per_g\n",
    "df_per_g.columns.values[[7, 8 , 9, 11, 12, 14, 15, 18, 19]] = \\\n",
    "[df_per_g.columns.values[[7, 8 , 9, 11, 12, 14, 15, 18, 19]][col] + \"_per_G\" for col in range(9)]\n",
    "\n",
    "df_per_g.columns.values[21:29] = [df_per_g.columns.values[21:30][col] + \\\n",
    "\"_per_G\" for col in range(8)]\n",
    "\n",
    "# Rename PS/G to PTS_per_G\n",
    "df_per_g.rename(columns={'PS/G': 'PTS_per_G'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_36m.columns.values[[7, 8, 9, 11, 12, 14, 15, 18, 19]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if proper values were changed\n",
    "list(df_per_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add per_36m to its column values\n",
    "df_per_36m.columns.values[[8, 9, 11, 12, 14, 15, 17, 18]] = \\\n",
    "[df_per_36m.columns.values[[8, 9, 11, 12, 14, 15, 17, 18]][col] + \"_per_36m\" \\\n",
    "for col in range(8)]\n",
    "\n",
    "df_per_36m.columns.values[20:30] = [df_per_36m.columns.values[20:30][col] + \"_per_36m\" \\\n",
    "                                   for col in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns were changed properly\n",
    "list(df_per_36m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add per_100p to per 100 possession column values\n",
    "list_of_changes = ['FG', 'FGA', '3P', '3PA', '2P', '2PA', 'FT', 'FTA', 'ORB', 'DRB', 'TRB', 'AST', 'STL', \\\n",
    "                   'BLK', 'TOV', 'PF', 'PTS']\n",
    "# Grab a list of current column names\n",
    "column_values = list(df_per_100p.columns.values)\n",
    "\n",
    "# Create a list for updated column names\n",
    "updated_columns = []\n",
    "\n",
    "# Loop through original column values to see what to replace\n",
    "for value in column_values:\n",
    "    if value in list_of_changes:\n",
    "        updated_columns.append(value + '_per_100p')\n",
    "    else:\n",
    "        updated_columns.append(value)\n",
    "\n",
    "# Update column values\n",
    "df_per_100p.columns = updated_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if columns are properly named\n",
    "list(df_per_100p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find where '\\xa0' columns are for removal\n",
    "print(df_adv.columns[-5])\n",
    "print(df_adv.columns[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop '\\xa0' columns, last one first\n",
    "#df_adv.drop(df_adv.columns[-5], axis = 1, inplace = True)\n",
    "#df_adv.drop(df_adv.columns[19], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adv.rename(columns = {'WS/48' : 'WS_per_48'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if columns were dropped properly\n",
    "list(df_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes later on season, player name, and team\n",
    "# Order of merges: tots, per_g, per_36m, adv\n",
    "# DFs: df_tot, df_per_g, df_per_36m, df_adv\n",
    "# Common things: Season, Player, Pos, Age, Tm, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(df_tot, df_per_g, how = \"left\", \n",
    "                 on = ['Season', 'Player', 'Pos', 'Age', 'Tm', 'G', 'GS', 'FT_perc',\n",
    "                      '3P_perc', '2P_perc', 'FG_perc', 'eFG_perc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(df_all, df_per_36m, how = \"left\",\n",
    "                 on = ['Season', 'Player', 'Pos', 'Age', 'Tm', 'G', 'GS', 'FT_perc',\n",
    "                      '3P_perc', '2P_perc', 'FG_perc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(df_all, df_adv, how = \"left\",\n",
    "                on = ['Season', 'Player', 'Pos', 'Age', 'Tm', 'G'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.merge(df_all, df_per_100p, how = \"left\",\n",
    "             on = ['Season', 'Player', 'Pos', 'Age', 'Tm', 'G', 'GS', \n",
    "                   'FG_perc', '3P_perc', '2P_perc', 'FT_perc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns to make sure they're all right\n",
    "list(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to drop duplicate MP columns\n",
    "list(df_all.drop(['MP_x', 'MP_y'], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.drop(['MP_x', 'MP_y'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final check of columns\n",
    "list(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check length of dataframe\n",
    "print(len(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill Null values with 0\n",
    "df_all.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address ambiguous positions and combination positions\n",
    "df = df_all.groupby(['Pos'])['Pos'].nunique()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove where 'Pos' value is 0\n",
    "df_all = df_all[df_all['Pos'] != 0]\n",
    "\n",
    "# Then check df_all length again\n",
    "print(len(df_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think the PG-SF and C-SF positions are mistakes\n",
    "# Check the value to see the player\n",
    "df_all[df_all['Pos'] == 'C-SF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Bobby Jones' actual, commonly played position\n",
    "df_all[df_all['Player'] == 'Bobby Jones']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of dual positions in DataFrame\n",
    "# Create empty DataFrame to audit dual position values\n",
    "column_names = list(df_all.columns.values)\n",
    "dual_pos_rows = []\n",
    "df_dual_pos = pd.DataFrame(columns = column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all the dual positions by seeing which ones have a dash\n",
    "for pos in df_all['Pos']:\n",
    "    if \"-\" in pos:\n",
    "        if pos not in dual_pos_rows:\n",
    "            dual_pos_rows.append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all dual position rows to a new DataFrame for auditing\n",
    "for pos in dual_pos_rows:\n",
    "    df_dual_pos = df_dual_pos.append(df_all[df_all['Pos'] == pos],\n",
    "                                    ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dual_pos\n",
    "# It looks like all these players moved teams before\n",
    "# Certain players have multiple positions or changed positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dual_pos.groupby(['Player']).size().reset_index(name = 'Count').sort_values(['Count'], ascending = False).head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check what is going on with some players with multiple positions\n",
    "df_all[df_all['Player'] == 'Allen Iverson*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common position for this player\n",
    "df_all[df_all['Player'] == 'Allen Iverson*']\\\n",
    ".groupby(['Pos']).size().reset_index(name = 'Count')\\\n",
    ".sort_values(['Count'], ascending = False).iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of seasons played at most common position\n",
    "df_all[df_all['Player'] == 'Allen Iverson*'].groupby(['Pos']).size().iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dictionary as key to replace 'Pos' values in the big DataFrame\n",
    "most_common_pos = {}\n",
    "# Saves a dictionary of player names with equally common positions\n",
    "two_common_pos = {}\n",
    "# PG, SG, SF, PF, C are 1-5, respectively\n",
    "pos_key = {'PG': '1', 'SG': '2', 'SF': '3', 'PF': '4', 'C': '5'}\n",
    "\n",
    "# Side note: This makes Tim Duncan a center\n",
    "\n",
    "def grab_most_common_pos(df, pos_dict):\n",
    "    # Loop through a dataframe and assign names and most common positions to a dictionary\n",
    "    for index, row in df[['Player', 'Pos']].iterrows():\n",
    "        player_name = row.iloc[0] # Assign player name to variable\n",
    "        # subset position dataframe to a player\n",
    "        pos_df = df[df['Player'] == player_name].groupby('Pos').size()\\\n",
    "        .reset_index(name = 'Count')\\\n",
    "        .sort_values(['Count'], ascending = False) \n",
    "        \n",
    "        #dual_pos_dict = {} # Store dict of dual positions\n",
    "        \n",
    "        pos = pos_df.iloc[0][0] # Assign first position to variable\n",
    "        second_pos = '' \n",
    "        \n",
    "        # Fill in second position if it exists\n",
    "        if len(pos_df) > 1:\n",
    "            second_pos = pos_df.iloc[1][0]    \n",
    "            \n",
    "        # Check is player has a second common position\n",
    "        # I don't know what to do in this situation yet\n",
    "        #if pos_df.iloc[0][1] == pos_df.iloc[1][1]:\n",
    "        #    dual_pos_dict['First position'] = pos\n",
    "        #    dual_pos_dict['Second position'] = second_pos\n",
    "        #    two_common_pos[player_name] = dual_pos_dict\n",
    "        #print(player_name)\n",
    "        \n",
    "        if player_name not in pos_dict.keys(): # Check if name exists first\n",
    "            pos_dict[player_name] = pos\n",
    "    \n",
    "    #return pos_dict\n",
    "\n",
    "def clean_pos(df, pos_dict):\n",
    "    # Loop through rows to check players' positions\n",
    "    grab_most_common_pos(df, pos_dict)\n",
    "    \n",
    "    # If the most common position is a dual position, take the first one\n",
    "    for name, pos in pos_dict.items():\n",
    "        if '-' in pos:\n",
    "            index = pos.find('-')\n",
    "            pos_dict[name] = pos[:index] \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    # Change pos_dict values to 1-5 from key\n",
    "    for key, value in pos_dict.items():\n",
    "        pos_dict[key] = pos_key[value]\n",
    "    \n",
    "    # Return DataFrame with cleaned positions\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes in a DataFrame and adds a new column with Rounded Position values\n",
    "def assign_pos(df, pos_dict):    \n",
    "    # Add a Rounded_Pos column and fill it from pos_dict\n",
    "    df['Rounded_Pos'] = ''\n",
    "    \n",
    "    for name, pos in pos_dict.items():\n",
    "        df.Rounded_Pos[df['Player'] == name] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pos(df_all, most_common_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assign_pos(df_all, most_common_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to csv file for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with top 25 single season scorers \n",
    "#df_top_25_scorers = df_all.sort_values('PTS_per_G', ascending = False).head(n=25)\n",
    "\n",
    "# Create a DataFrame with top 50 single season scorers \n",
    "#df_top_50_scorers = df_all.sort_values('PTS_per_G', ascending = False).head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplayer_data_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(user_start_year) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(user_end_year) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#print(file_name)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdf_all\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(file_name, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_all' is not defined"
     ]
    }
   ],
   "source": [
    "# Write to CSV files and DONE!\n",
    "file_name = 'player_data_' + str(user_start_year) + '-' + str(user_end_year) + '.csv'\n",
    "#print(file_name)\n",
    "df_all.to_csv(file_name, encoding = 'utf-8', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_top_50_scorers.to_csv(\"bref_1981_2017_top_50_season_scorers.csv\", encoding = \"utf-8\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
